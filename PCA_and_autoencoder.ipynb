{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e133a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_wdbc(path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(',')\n",
    "            labels.append(1 if parts[1] == 'M' else 0)\n",
    "            features = list(map(float, parts[2:]))\n",
    "            data.append(features)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "X, y = load_wdbc(\"wdbc.data\")\n",
    "\n",
    "print(X.shape)  # (569, 30)\n",
    "print(y.shape)  # (569,)\n",
    "\n",
    "def standardize(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    return (X - mean) / (std + 1e-8)\n",
    "\n",
    "X = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ec3a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.mean = None\n",
    "        self.components = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # 1. Center data\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # 2. Covariance matrix\n",
    "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        # 3. Eigen decomposition\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "        # 4. Sort eigenvalues & eigenvectors (descending)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # 5. Select top components\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "\n",
    "        # 6. Explained variance ratio\n",
    "        total_variance = np.sum(eigenvalues)\n",
    "        self.explained_variance_ratio_ = eigenvalues[:self.n_components] / total_variance\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "    def inverse_transform(self, X_pca):\n",
    "        return np.dot(X_pca, self.components.T) + self.mean\n",
    "\n",
    "    def reconstruction_error(self, X):\n",
    "        X_pca = self.transform(X)\n",
    "        X_reconstructed = self.inverse_transform(X_pca)\n",
    "        return np.mean((X - X_reconstructed) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe077068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.44272051 0.1897117 ]\n",
      "Reconstruction error: 0.3675674130150981\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform(X)\n",
    "error = pca.reconstruction_error(X)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Reconstruction error:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e23e55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05be1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    def __init__(self, layer_sizes, activation='relu', lr=0.01, l2=0.001):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.lr = lr\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.activations = {\n",
    "            'relu': (relu, relu_derivative),\n",
    "            'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "            'tanh': (tanh, tanh_derivative)\n",
    "        }\n",
    "\n",
    "        self.f, self.f_prime = self.activations[activation]\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(\n",
    "                np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            )\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.zs = []\n",
    "        self.activations_cache = [X]\n",
    "\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(self.activations_cache[-1], W) + b\n",
    "            self.zs.append(z)\n",
    "            a = self.f(z)\n",
    "            self.activations_cache.append(a)\n",
    "\n",
    "        return self.activations_cache[-1]\n",
    "\n",
    "    def backward(self, X):\n",
    "        m = X.shape[0]\n",
    "        grads_W = []\n",
    "        grads_b = []\n",
    "\n",
    "        # MSE loss derivative\n",
    "        delta = (self.activations_cache[-1] - X) * self.f_prime(self.zs[-1])\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dW = np.dot(self.activations_cache[i].T, delta) / m\n",
    "            db = np.mean(delta, axis=0, keepdims=True)\n",
    "\n",
    "            # L2 regularization\n",
    "            dW += self.l2 * self.weights[i]\n",
    "\n",
    "            grads_W.insert(0, dW)\n",
    "            grads_b.insert(0, db)\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.f_prime(self.zs[i-1])\n",
    "\n",
    "        return grads_W, grads_b\n",
    "\n",
    "    def update(self, grads_W, grads_b):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.lr * grads_W[i]\n",
    "            self.biases[i] -= self.lr * grads_b[i]\n",
    "\n",
    "    def train(self, X, epochs=100, batch_size=32, decay=0.99):\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(len(X))\n",
    "            X_shuffled = X[indices]\n",
    "\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                self.forward(X_batch)\n",
    "                grads_W, grads_b = self.backward(X_batch)\n",
    "                self.update(grads_W, grads_b)\n",
    "\n",
    "            self.lr *= decay  # learning rate scheduling\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((self.forward(X) - X) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846e139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.00000\n",
      "Epoch 10, Loss: 1.00000\n",
      "Epoch 20, Loss: 1.00000\n",
      "Epoch 30, Loss: 1.00000\n",
      "Epoch 40, Loss: 1.00000\n",
      "Epoch 50, Loss: 1.00000\n",
      "Epoch 60, Loss: 1.00000\n",
      "Epoch 70, Loss: 1.00000\n",
      "Epoch 80, Loss: 1.00000\n",
      "Epoch 90, Loss: 1.00000\n",
      "Epoch 100, Loss: 1.00000\n",
      "Epoch 110, Loss: 1.00000\n",
      "Epoch 120, Loss: 1.00000\n",
      "Epoch 130, Loss: 1.00000\n",
      "Epoch 140, Loss: 1.00000\n",
      "Epoch 150, Loss: 1.00000\n",
      "Epoch 160, Loss: 1.00000\n",
      "Epoch 170, Loss: 1.00000\n",
      "Epoch 180, Loss: 1.00000\n",
      "Epoch 190, Loss: 1.00000\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [30, 20, 10, 2, 10, 20, 30]\n",
    "ae = Autoencoder(layer_sizes, activation='relu', lr=0.01, l2=0.001)\n",
    "\n",
    "ae.train(X, epochs=200, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
